{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0137380e",
   "metadata": {},
   "source": [
    "---\n",
    "# Analisis Putusan Perdata Khusus Merek Menggunakan Case-Based Reasoning Berbasis TF-IDF: Studi Perbandingan SVM dan Naive Bayes\n",
    "\n",
    "Anggota Kelompok :\n",
    "\n",
    "1. Adam Kurniawan - 202210370311211\n",
    "2. Wafiq Azizah - 202210370311247\n",
    "\n",
    "## Tugas Besar Mata Kuliah Penalaran Komputer (B)\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b681675",
   "metadata": {},
   "source": [
    "## Tahap 1 – Membangun Case Base\n",
    "\n",
    "### Tujuan\n",
    "Tahap ini bertujuan untuk mengumpulkan dan membersihkan dokumen putusan pengadilan perdata khusus merek untuk membentuk basis kasus yang terstruktur sebagai fondasi proses reasoning dalam sistem Case-Based Reasoning.\n",
    "\n",
    "---\n",
    "\n",
    "### Langkah Kerja\n",
    "\n",
    "#### 1. Seleksi dan Pengunduhan Dokumen\n",
    "\n",
    "- Jenis perkara yang dipilih adalah **Perdata Khusus terkait Merek**.\n",
    "- Data diambil melalui proses scraping dari situs resmi putusan Mahkamah Agung.\n",
    "- Dokumen hasil unduhan disimpan dalam format PDF pada folder `Merek_PDF/`.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Konversi dan Ekstraksi Teks\n",
    "\n",
    "- File PDF diubah menjadi format teks dengan bantuan library `pdfminer`.\n",
    "- Setiap dokumen diproses untuk mengekstrak keseluruhan isi putusan dalam bentuk teks mentah.\n",
    "- Hasil ekstraksi disimpan secara terpisah untuk setiap dokumen di direktori `data/raw/`.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Pembersihan Teks (Cleaning)\n",
    "\n",
    "- Elemen seperti watermark, header, footer, dan disclaimer resmi dari Mahkamah Agung dihapus dari dokumen.\n",
    "- Proses normalisasi teks mencakup:\n",
    "  - Penghapusan baris kosong yang berulang.\n",
    "  - Konversi seluruh huruf menjadi huruf kecil.\n",
    "  - Penghilangan spasi yang tidak perlu.\n",
    "- Rasio retensi konten digunakan untuk menilai sejauh mana informasi utama tetap terjaga setelah proses pembersihan.\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. Validasi dan Logging\n",
    "\n",
    "- Setiap file yang telah diproses dicatat dalam log dengan informasi berikut:  \n",
    "  - Nomor dokumen\n",
    "  - Status hasil pembersihan (OK atau rendah)\n",
    "  - Direktori penyimpanan\n",
    "  - Waktu dan tanggal pemrosesan\n",
    "- Seluruh catatan log disimpan dalam file `logs/cleaning.log`.\n",
    "\n",
    "---\n",
    "\n",
    "### Output Tahap Ini\n",
    "\n",
    "- Jumlah total dokumen yang berhasil diproses sebanyak **70 kasus**.\n",
    "- Teks hasil pembersihan disimpan di direktori `data/raw/`.\n",
    "- Informasi proses dan status pembersihan tercatat dalam file log `logs/cleaning.log`.\n",
    "\n",
    "---\n",
    "\n",
    "Tahap awal ini berhasil menghasilkan korpus dasar untuk mendukung sistem CBR. Lebih dari 80% dokumen telah berhasil dikonversi ke teks dan dibersihkan dengan metode terstruktur, sehingga siap digunakan pada proses representasi berikutnya.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6134879d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning PDF text extraction...\n",
      "[OK] case_001 processed (96.43% valid).\n",
      "[OK] case_002 processed (96.25% valid).\n",
      "[OK] case_003 processed (96.56% valid).\n",
      "[OK] case_004 processed (96.38% valid).\n",
      "[OK] case_005 processed (96.34% valid).\n",
      "[OK] case_006 processed (96.41% valid).\n",
      "[OK] case_007 processed (96.38% valid).\n",
      "[OK] case_008 processed (96.22% valid).\n",
      "[OK] case_009 processed (96.46% valid).\n",
      "[OK] case_010 processed (96.53% valid).\n",
      "[OK] case_011 processed (96.10% valid).\n",
      "[OK] case_012 processed (96.40% valid).\n",
      "[OK] case_013 processed (96.59% valid).\n",
      "[OK] case_014 processed (96.26% valid).\n",
      "[OK] case_015 processed (96.03% valid).\n",
      "[OK] case_016 processed (96.59% valid).\n",
      "[OK] case_017 processed (96.82% valid).\n",
      "[OK] case_018 processed (96.26% valid).\n",
      "[OK] case_019 processed (96.38% valid).\n",
      "[OK] case_020 processed (96.53% valid).\n",
      "[OK] case_021 processed (96.29% valid).\n",
      "[OK] case_022 processed (96.28% valid).\n",
      "[OK] case_023 processed (96.16% valid).\n",
      "[OK] case_024 processed (96.64% valid).\n",
      "[OK] case_025 processed (96.10% valid).\n",
      "[OK] case_026 processed (96.43% valid).\n",
      "[OK] case_027 processed (96.63% valid).\n",
      "[OK] case_028 processed (96.27% valid).\n",
      "[OK] case_029 processed (96.29% valid).\n",
      "[OK] case_030 processed (96.41% valid).\n",
      "[OK] case_031 processed (96.04% valid).\n",
      "[OK] case_032 processed (96.65% valid).\n",
      "[OK] case_033 processed (96.49% valid).\n",
      "[OK] case_034 processed (96.38% valid).\n",
      "[OK] case_035 processed (96.37% valid).\n",
      "[OK] case_036 processed (96.38% valid).\n",
      "[OK] case_037 processed (96.36% valid).\n",
      "[OK] case_038 processed (96.32% valid).\n",
      "[OK] case_039 processed (96.31% valid).\n",
      "[OK] case_040 processed (96.34% valid).\n",
      "[OK] case_041 processed (96.51% valid).\n",
      "[OK] case_042 processed (96.31% valid).\n",
      "[OK] case_043 processed (96.43% valid).\n",
      "[OK] case_044 processed (96.50% valid).\n",
      "[OK] case_045 processed (96.52% valid).\n",
      "[OK] case_046 processed (96.36% valid).\n",
      "[OK] case_047 processed (96.34% valid).\n",
      "[OK] case_048 processed (96.43% valid).\n",
      "[OK] case_049 processed (96.45% valid).\n",
      "[OK] case_050 processed (95.84% valid).\n",
      "[OK] case_051 processed (96.27% valid).\n",
      "[OK] case_052 processed (96.22% valid).\n",
      "[OK] case_053 processed (96.21% valid).\n",
      "[OK] case_054 processed (96.57% valid).\n",
      "[OK] case_055 processed (96.36% valid).\n",
      "[OK] case_056 processed (96.37% valid).\n",
      "[OK] case_057 processed (96.47% valid).\n",
      "[OK] case_058 processed (96.37% valid).\n",
      "[OK] case_059 processed (96.09% valid).\n",
      "[OK] case_060 processed (96.32% valid).\n",
      "[OK] case_061 processed (96.53% valid).\n",
      "[OK] case_062 processed (96.52% valid).\n",
      "[OK] case_063 processed (96.41% valid).\n",
      "[OK] case_064 processed (96.40% valid).\n",
      "[OK] case_065 processed (96.45% valid).\n",
      "[OK] case_066 processed (96.30% valid).\n",
      "[OK] case_067 processed (96.39% valid).\n",
      "[OK] case_068 processed (95.95% valid).\n",
      "[OK] case_069 processed (96.46% valid).\n",
      "[OK] case_070 processed (96.54% valid).\n",
      "PDF processing completed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "from pdfminer.high_level import extract_text\n",
    "from datetime import datetime\n",
    "\n",
    "# === Directory configuration ===\n",
    "SOURCE_DIR = 'Merek_PDF'\n",
    "DEST_DIR = 'data/raw'\n",
    "LOG_DEST = 'logs/cleaning.log'\n",
    "\n",
    "def purify_text(input_text):\n",
    "    \"\"\"Remove unwanted headers, footers, and disclaimers from extracted text.\"\"\"\n",
    "    initial_length = len(input_text) if input_text else 1  # Prevent division by zero\n",
    "\n",
    "    # Eliminate watermarks and page indicators\n",
    "    processed_text = input_text\n",
    "    processed_text = processed_text.replace(\"Direktori Putusan Mahkamah Agung Republik Indonesia\", \"\")\n",
    "    processed_text = processed_text.replace(\"putusan.mahkamahagung.go.id\", \"\")\n",
    "    processed_text = re.sub(r'halaman\\s*\\d+', '', processed_text, flags=re.IGNORECASE)\n",
    "    processed_text = processed_text.replace(\"M a h ka m a h A g u n g R e p u blik In d o n esia\\n\", \"\")\n",
    "    \n",
    "    # Record length after initial cleaning\n",
    "    intermediate_length = len(processed_text)\n",
    "    \n",
    "    # Remove disclaimer content\n",
    "    processed_text = processed_text.replace(\"Disclaimer\\n\", \"\")\n",
    "    processed_text = processed_text.replace(\n",
    "        \"Kepaniteraan Mahkamah Agung Republik Indonesia berusaha untuk selalu mencantumkan informasi paling kini dan akurat sebagai bentuk komitmen Mahkamah Agung untuk pelayanan publik, transparansi dan akuntabilitas\\n\", \"\")\n",
    "    processed_text = processed_text.replace(\n",
    "        \"pelaksanaan fungsi peradilan. Namun dalam hal-hal tertentu masih dimungkinkan terjadi permasalahan teknis terkait dengan akurasi dan keterkinian informasi yang kami sajikan, hal mana akan terus kami perbaiki dari waktu kewaktu.\\n\", \"\")\n",
    "    processed_text = processed_text.replace(\n",
    "        \"Dalam hal Anda menemukan inakurasi informasi yang termuat pada situs ini atau informasi yang seharusnya ada, namun belum tersedia, maka harap segera hubungi Kepaniteraan Mahkamah Agung RI melalui :\\n\", \"\")\n",
    "    processed_text = processed_text.replace(\n",
    "        \"Email : kepaniteraan@mahkamahagung.go.id    Telp : 021-384 3348 (ext.318)\\n\", \"\")\n",
    "\n",
    "    # Final text normalization\n",
    "    processed_text = processed_text.lower()\n",
    "    processed_text = ' '.join(processed_text.split())\n",
    "\n",
    "    # Calculate content retention ratio\n",
    "    retention_ratio = intermediate_length / initial_length\n",
    "\n",
    "    return processed_text, retention_ratio\n",
    "\n",
    "def record_log(case_id, retention_ratio):\n",
    "    \"\"\"Log processing details to a file.\"\"\"\n",
    "    os.makedirs(os.path.dirname(LOG_DEST), exist_ok=True)\n",
    "    with open(LOG_DEST, 'a', encoding='utf-8') as log:\n",
    "        timestamp = datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "        log.write(f\"[{timestamp}] {case_id} | Content Integrity: {retention_ratio:.2%}\\n\")\n",
    "\n",
    "def handle_pdf_batch():\n",
    "    \"\"\"Process all PDFs in the source directory.\"\"\"\n",
    "    os.makedirs(DEST_DIR, exist_ok=True)\n",
    "    pdf_paths = sorted(glob.glob(os.path.join(SOURCE_DIR, '*.pdf')))\n",
    "    \n",
    "    if not pdf_paths:\n",
    "        print(\"No PDFs found in the source directory.\")\n",
    "        return\n",
    "\n",
    "    for index, pdf in enumerate(pdf_paths, 1):\n",
    "        case_id = f\"case_{index:03d}\"\n",
    "        output_path = os.path.join(DEST_DIR, f\"{case_id}.txt\")\n",
    "\n",
    "        try:\n",
    "            # Extract text from PDF\n",
    "            raw_text = extract_text(pdf)\n",
    "\n",
    "            # Clean and validate text\n",
    "            final_text, integrity_ratio = purify_text(raw_text)\n",
    "\n",
    "            # Check content integrity\n",
    "            if integrity_ratio < 0.8:\n",
    "                print(f\"[WARNING] {case_id} retains only {integrity_ratio:.2%} of content.\")\n",
    "            else:\n",
    "                print(f\"[OK] {case_id} processed ({integrity_ratio:.2%} valid).\")\n",
    "                # Save cleaned text if valid\n",
    "                with open(output_path, 'w', encoding='utf-8') as output:\n",
    "                    output.write(final_text)\n",
    "\n",
    "            # Log the result\n",
    "            record_log(case_id, integrity_ratio)\n",
    "\n",
    "        except Exception as err:\n",
    "            print(f\"[ERROR] Failed to process {pdf}: {str(err)}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Initiate PDF processing workflow.\"\"\"\n",
    "    print(\"Beginning PDF text extraction...\")\n",
    "    handle_pdf_batch()\n",
    "    print(\"PDF processing completed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ce0de1",
   "metadata": {},
   "source": [
    "## Tahap 2 – Case Representation\n",
    "\n",
    "### Tujuan\n",
    "\n",
    "Mewakili setiap dokumen putusan dalam bentuk vektor fitur menggunakan pendekatan TF-IDF agar dapat digunakan sebagai input dalam sistem Case-Based Reasoning, serta mempersiapkan data untuk proses klasifikasi menggunakan algoritma SVM dan Naive Bayes.\n",
    "\n",
    "---\n",
    "\n",
    "### Langkah Kerja\n",
    "\n",
    "#### 1. Ekstraksi Metadata\n",
    "\n",
    "Setiap dokumen hasil pembersihan dianalisis untuk mengambil informasi penting sebagai metadata menggunakan metode berbasis pola (regex) dan teknik pemrosesan teks. Metadata yang diambil mencakup:\n",
    "\n",
    "- Nomor perkara (`no_perkara`)\n",
    "- Tanggal putusan (`tanggal`)\n",
    "- Ringkasan fakta hukum (`ringkasan_fakta`)\n",
    "- Dasar hukum atau pasal yang digunakan (`pasal`)\n",
    "- Identitas pihak-pihak yang bersengketa (penggugat dan tergugat)\n",
    "- Konten lengkap putusan (`text_full`)\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Penyimpanan Data Terstruktur\n",
    "\n",
    "Output metadata yang telah diekstraksi disimpan dalam dua format file guna mempermudah tahap pemrosesan selanjutnya:\n",
    "\n",
    "- **CSV** disimpan di: `data/processed/cases_extracted.csv`\n",
    "- **JSON** disimpan di: `data/processed/cases_extracted.json`\n",
    "\n",
    "Setiap entri kasus memiliki struktur data yang terdiri dari beberapa atribut, yaitu:\n",
    "\n",
    "- `case_id`\n",
    "- `no_perkara`\n",
    "- `tanggal`\n",
    "- `ringkasan_fakta`\n",
    "- `pasal`\n",
    "- `pihak`\n",
    "- `text_full`\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Feature Engineering\n",
    "\n",
    "Untuk memperkaya kualitas representasi setiap kasus, dilakukan tahapan rekayasa fitur (feature engineering) dengan beberapa pendekatan berikut:\n",
    "\n",
    "- **Panjang Teks**: Mengukur jumlah kata yang terdapat dalam kolom `ringkasan_fakta`.\n",
    "- **Bag-of-Words (BoW)**: Mengidentifikasi dan menghitung frekuensi kemunculan kata dalam ringkasan untuk membentuk representasi fitur.\n",
    "- **Pasangan QA Sederhana**: Menyusun pertanyaan dan jawaban otomatis berdasarkan informasi dalam ringkasan, contohnya:\n",
    "\n",
    "  - Berapa nomor perkara dalam kasus ini?\n",
    "  - Dasar hukum atau pasal apa yang diterapkan?\n",
    "  - Siapa yang menjadi tergugat dalam perkara tersebut?\n",
    "  - Bagaimana ringkasan fakta dari kasus ini?\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. Penyimpanan Fitur\n",
    "\n",
    "Fitur-fitur hasil rekayasa disimpan dalam format JSON untuk memudahkan penggunaan dalam tahap analisis berikutnya, dengan rincian sebagai berikut:\n",
    "\n",
    "- `data/processed/features_length.json` – berisi informasi panjang teks\n",
    "- `data/processed/features_bow.json` – mencakup representasi frekuensi kata\n",
    "- `data/processed/features_qa_pairs.json` – menyimpan pasangan pertanyaan dan jawaban dari ringkasan\n",
    "\n",
    "---\n",
    "\n",
    "Tahap representasi berhasil menyusun data setiap kasus dalam format yang terstruktur dan sistematis. Seluruh kasus kini dilengkapi dengan metadata utama serta fitur tambahan yang dirancang untuk memperkuat proses penelusuran dan penalaran pada tahap selanjutnya dalam sistem Case-Based Reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ac37627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUKSES] 70 kasus berhasil diproses dan disimpan:\n",
      "CSV → data/processed\\cases_extracted.csv\n",
      "JSON → data/processed\\cases_extracted.json\n",
      "Length → data/processed\\features_length.json\n",
      "BoW → data/processed\\features_bow.json\n",
      "QA Pairs → data/processed\\features_qa_pairs.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import csv\n",
    "from collections import Counter\n",
    "\n",
    "RAW_FOLDER = 'data/raw'\n",
    "PROCESSED_FOLDER = 'data/processed'\n",
    "\n",
    "os.makedirs(PROCESSED_FOLDER, exist_ok=True)\n",
    "\n",
    "def extract_metadata(text):\n",
    "    metadata = {}\n",
    "\n",
    "    # Nomor Perkara\n",
    "    match = re.search(r'Nomor\\s*:\\s*(\\S+)', text, re.IGNORECASE)\n",
    "    metadata['no_perkara'] = match.group(1) if match else ''\n",
    "\n",
    "    # Tanggal Putusan\n",
    "    match = re.search(r'\\b(?:putusan|diputuskan)\\s*(?:pada|tanggal)?\\s*(\\d{1,2}\\s+\\w+\\s+\\d{4})', text, re.IGNORECASE)\n",
    "    metadata['tanggal'] = match.group(1) if match else ''\n",
    "\n",
    "    # Pasal yang disebutkan\n",
    "    match = re.findall(r'Pasal\\s+\\d+\\s+[^\\n.,;]*', text, re.IGNORECASE)\n",
    "    metadata['pasal'] = match if match else []\n",
    "\n",
    "    # Ringkasan Fakta Sederhana\n",
    "    match = re.search(r'(menimbang\\s+bahwa.*?)((?:menimbang|mengingat|memperhatikan)\\b.*)', text, re.IGNORECASE | re.DOTALL)\n",
    "    metadata['ringkasan_fakta'] = match.group(1).strip() if match else ''\n",
    "\n",
    "    # Terdakwa & Korban (opsional)\n",
    "    terdakwa = re.findall(r'terdakwa(?:\\s*:\\s*|\\s+)([A-Z][a-zA-Z\\s]+)', text)\n",
    "    korban = re.findall(r'korban(?:\\s*:\\s*|\\s+)([A-Z][a-zA-Z\\s]+)', text)\n",
    "    metadata['pihak'] = {\n",
    "        'terdakwa': terdakwa[0] if terdakwa else '',\n",
    "        'korban': korban[0] if korban else ''\n",
    "    }\n",
    "\n",
    "    return metadata\n",
    "\n",
    "def feature_engineering(text, metadata):\n",
    "    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    length = len(tokens)\n",
    "    bow = dict(Counter(tokens))\n",
    "\n",
    "    qa_pairs = {\n",
    "        \"Berapa nomor perkara dalam kasus ini?\": metadata.get(\"no_perkara\", \"\"),\n",
    "        \"Dasar hukum atau pasal apa yang diterapkan?\": \", \".join(metadata.get(\"pasal\", [])),\n",
    "        \"Siapa yang menjadi tergugat dalam perkara tersebut?\": metadata.get(\"pihak\", {}).get(\"terdakwa\", \"\"),\n",
    "        \"Bagaimana ringkasan fakta dari kasus ini?\": metadata.get(\"pihak\", {}).get(\"korban\", \"\")\n",
    "    }\n",
    "\n",
    "    return length, bow, qa_pairs\n",
    "\n",
    "def process_all():\n",
    "    # === Cek folder dan file .txt ===\n",
    "    if not os.path.exists(RAW_FOLDER):\n",
    "        raise FileNotFoundError(f\"[ERROR] Folder '{RAW_FOLDER}' tidak ditemukan. Harap buat dan isi dengan file .txt.\")\n",
    "\n",
    "    txt_files = sorted([f for f in os.listdir(RAW_FOLDER) if f.endswith('.txt')])\n",
    "    if not txt_files:\n",
    "        raise FileNotFoundError(f\"[ERROR] Tidak ada file .txt dalam '{RAW_FOLDER}'. Harap isi folder dengan putusan .txt.\")\n",
    "\n",
    "    all_cases = []\n",
    "    features_length = {}\n",
    "    features_bow = {}\n",
    "    features_qa = {}\n",
    "\n",
    "    for i, fname in enumerate(txt_files, 1):\n",
    "        path = os.path.join(RAW_FOLDER, fname)\n",
    "        with open(path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read().strip()\n",
    "\n",
    "        if not text:\n",
    "            print(f\"[PERINGATAN] File kosong dilewati: {fname}\")\n",
    "            continue\n",
    "\n",
    "        case_id = f\"case_{i:03d}\"\n",
    "        metadata = extract_metadata(text)\n",
    "        metadata['case_id'] = case_id\n",
    "        metadata['text_full'] = text\n",
    "\n",
    "        length, bow, qa = feature_engineering(text, metadata)\n",
    "\n",
    "        all_cases.append(metadata)\n",
    "        features_length[case_id] = length\n",
    "        features_bow[case_id] = bow\n",
    "        features_qa[case_id] = qa\n",
    "\n",
    "    # Simpan CSV\n",
    "    csv_path = os.path.join(PROCESSED_FOLDER, 'cases_extracted.csv')\n",
    "    with open(csv_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        fieldnames = ['case_id', 'no_perkara', 'tanggal', 'ringkasan_fakta', 'pasal', 'pihak', 'text_full']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for row in all_cases:\n",
    "            writer.writerow(row)\n",
    "\n",
    "    # Simpan JSON\n",
    "    json_path = os.path.join(PROCESSED_FOLDER, 'cases_extracted.json')\n",
    "    with open(json_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(all_cases, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # Simpan fitur\n",
    "    with open(os.path.join(PROCESSED_FOLDER, 'features_length.json'), 'w', encoding='utf-8') as f:\n",
    "        json.dump(features_length, f, ensure_ascii=False, indent=2)\n",
    "    with open(os.path.join(PROCESSED_FOLDER, 'features_bow.json'), 'w', encoding='utf-8') as f:\n",
    "        json.dump(features_bow, f, ensure_ascii=False, indent=2)\n",
    "    with open(os.path.join(PROCESSED_FOLDER, 'features_qa_pairs.json'), 'w', encoding='utf-8') as f:\n",
    "        json.dump(features_qa, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"[SUKSES] {len(all_cases)} kasus berhasil diproses dan disimpan:\")\n",
    "    print(\"CSV →\", csv_path)\n",
    "    print(\"JSON →\", json_path)\n",
    "    print(\"Length →\", os.path.join(PROCESSED_FOLDER, 'features_length.json'))\n",
    "    print(\"BoW →\", os.path.join(PROCESSED_FOLDER, 'features_bow.json'))\n",
    "    print(\"QA Pairs →\", os.path.join(PROCESSED_FOLDER, 'features_qa_pairs.json'))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_all()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57bb811f",
   "metadata": {},
   "source": [
    "## Tahap 3 – Case Retrieval\n",
    "\n",
    "### Tujuan\n",
    "\n",
    "Mengembangkan mekanisme pencarian kasus serupa berdasarkan kemiripan teks menggunakan representasi TF-IDF, untuk menemukan putusan terdahulu yang relevan sebagai referensi dalam sistem Case-Based Reasoning, serta mengevaluasi kinerja pencarian dengan algoritma SVM dan Naive Bayes.\n",
    "\n",
    "---\n",
    "\n",
    "### Langkah Kerja\n",
    "\n",
    "#### 1. Representasi Vektor\n",
    "\n",
    "Setiap ringkasan fakta dari putusan diolah menjadi vektor numerik dengan pendekatan **TF-IDF (Term Frequency – Inverse Document Frequency)**. Konversi ini bertujuan untuk merepresentasikan teks dalam format angka sehingga dapat digunakan dalam perhitungan tingkat kemiripan antar dokumen dan proses klasifikasi lanjutan.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Splitting Data\n",
    "\n",
    "Dataset dibagi ke dalam dua subset, yaitu:\n",
    "\n",
    "- **Data pelatihan (training set)** sebesar 80%\n",
    "- **Data pengujian (test set)** sebesar 20%\n",
    "\n",
    "Pembagian ini digunakan untuk melatih model klasifikasi yang menggabungkan TF-IDF dengan algoritma SVM, serta untuk melakukan evaluasi awal terhadap efektivitas proses pencarian kasus serupa.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Model Retrieval\n",
    "\n",
    "Terdapat dua metode yang digunakan dalam proses pencarian kasus serupa (retrieval):\n",
    "\n",
    "- **a. TF-IDF + Cosine Similarity (Pendekatan CBR)**\n",
    "  Mengukur kemiripan antara vektor TF-IDF dari query dan seluruh kasus dalam basis data menggunakan cosine similarity. Kasus-kasus dengan skor tertinggi (top-k) dianggap paling relevan.\n",
    "\n",
    "- **b. TF-IDF + Support Vector Machine (SVM)**\n",
    "  Menggunakan pendekatan supervised learning, di mana model SVM dilatih untuk menghubungkan ringkasan fakta dengan `case_id`. Ketika menerima sebuah query, model akan memprediksi `case_id` yang paling cocok.\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. Fungsi Retrieval\n",
    "\n",
    "Fungsi pencarian (retrieval) dikembangkan secara terpisah untuk masing-masing metode:\n",
    "\n",
    "- `retrieve_cosine(query: str, k: int = 5)`\n",
    "  Menghasilkan top-k `case_id` dengan nilai kemiripan cosine tertinggi terhadap query dari seluruh basis kasus.\n",
    "\n",
    "- `retrieve_svm(query: str)`\n",
    "  Mengembalikan satu `case_id` hasil prediksi model SVM berdasarkan klasifikasi ringkasan fakta dari query yang diberikan.\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. Pengujian Awal\n",
    "\n",
    "Sebanyak 10 query uji disusun lengkap dengan `case_id` yang menjadi ground-truth. Untuk setiap query:\n",
    "\n",
    "- Dihitung top-5 hasil teratas berdasarkan skor cosine similarity.\n",
    "- Diprediksi satu hasil teratas menggunakan model SVM.\n",
    "- Kedua hasil tersebut kemudian dibandingkan dengan ground-truth untuk evaluasi akurasi.\n",
    "\n",
    "Seluruh data query uji disimpan dalam file:\n",
    "\n",
    "- `data/eval/queries.json`\n",
    "\n",
    "---\n",
    "\n",
    "### Output\n",
    "\n",
    "Tahap ini menghasilkan beberapa file penting sebagai output, yaitu:\n",
    "\n",
    "- Model klasifikasi SVM disimpan dalam file: `03_retrieval_model.pkl`\n",
    "- Objek TF-IDF vectorizer disimpan dalam: `03_vectorizer.pkl`\n",
    "- Dataset berisi query uji tersimpan di: `data/eval/queries.json`\n",
    "\n",
    "---\n",
    "\n",
    "Proses Case Retrieval telah selesai diterapkan dengan dua pendekatan berbeda: metode unsupervised berbasis kemiripan teks dan metode supervised melalui model klasifikasi. Keduanya telah disiapkan untuk mendukung proses evaluasi dan prediksi kasus pada tahapan selanjutnya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "550b633f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUKSES] Model disimpan di : 03_retrieval_model.pkl\n",
      "[SUKSES] Vectorizer disimpan di : 03_vectorizer.pkl\n",
      "[SUKSES] 10 query uji disimpan di : data/eval/queries.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\MATA KULIAH\\Semester 6\\Penalaran Komputer\\coba\\putusan_pidana-khusus\\.conda\\Lib\\site-packages\\sklearn\\utils\\multiclass.py:213: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
      "  y_type = type_of_target(y, input_name=\"y\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# === Konfigurasi path ===\n",
    "CSV_PATH = \"data/processed/cases_extracted.csv\"\n",
    "MODEL_PATH = \"03_retrieval_model.pkl\"\n",
    "VECTORIZER_PATH = \"03_vectorizer.pkl\"\n",
    "QUERY_PATH = \"data/eval/queries.json\"\n",
    "os.makedirs(\"data/eval\", exist_ok=True)\n",
    "\n",
    "# === Muat data CSV ===\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "texts = df['ringkasan_fakta'].fillna(df['text_full']).astype(str)\n",
    "labels = df['case_id'].astype(str)\n",
    "\n",
    "# === Split data 80:20 ===\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    texts, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# === TF-IDF + SVM ===\n",
    "vectorizer = TfidfVectorizer(max_features=3000, ngram_range=(1,2))\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "\n",
    "classifier = LinearSVC()\n",
    "classifier.fit(X_train_vec, y_train)\n",
    "\n",
    "# === Simpan model dan vectorizer ===\n",
    "joblib.dump(classifier, MODEL_PATH)\n",
    "joblib.dump(vectorizer, VECTORIZER_PATH)\n",
    "\n",
    "# === Ambil 10 query uji ===\n",
    "sample_df = df[df['ringkasan_fakta'].notnull() & (df['ringkasan_fakta'].str.strip() != '')]\n",
    "sample_df = sample_df.sample(n=10, random_state=42)\n",
    "\n",
    "sample_queries = [\n",
    "    {\n",
    "        \"query\": row[\"ringkasan_fakta\"][:500],\n",
    "        \"ground_truth\": row[\"case_id\"]\n",
    "    }\n",
    "    for _, row in sample_df.iterrows()\n",
    "]\n",
    "\n",
    "with open(QUERY_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(sample_queries, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"[SUKSES] Model disimpan di :\", MODEL_PATH)\n",
    "print(\"[SUKSES] Vectorizer disimpan di :\", VECTORIZER_PATH)\n",
    "print(\"[SUKSES] 10 query uji disimpan di :\", QUERY_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8a0faa",
   "metadata": {},
   "source": [
    "### Tahap 4 – Solution Reuse\n",
    "\n",
    "#### Tujuan\n",
    "\n",
    "Mengambil dan memanfaatkan solusi dari kasus terdahulu yang paling relevan berdasarkan hasil retrieval, untuk memberikan rekomendasi atau prediksi putusan pada kasus baru dalam sistem Case-Based Reasoning.\n",
    "\n",
    "---\n",
    "\n",
    "#### Langkah Kerja\n",
    "\n",
    "1. **Ekstraksi Solusi**\n",
    "   - Untuk setiap kasus terdahulu yang berhasil ditemukan pada tahap sebelumnya, bagian **amar putusan** atau segmen akhir dari dokumen diekstrak sebagai `solusi_text`.\n",
    "   - Informasi amar putusan ini disimpan dalam struktur data berupa pasangan `{case_id: solusi_text}`.\n",
    "\n",
    "2. **Algoritma Prediksi Solusi**\n",
    "\n",
    "Dua pendekatan utama digunakan dalam proses prediksi solusi:\n",
    "\n",
    "- **CBR (TF-IDF + Cosine Similarity)**\n",
    "  Sistem menyeleksi *top-k* kasus yang paling mirip berdasarkan nilai cosine similarity, kemudian menerapkan salah satu dari tiga strategi berikut:\n",
    "\n",
    "  - `weighted`: memilih solusi dari kasus dengan akumulasi skor similarity tertinggi.\n",
    "  - `majority`: menentukan solusi berdasarkan frekuensi kemunculan terbanyak di antara *top-k*.\n",
    "  - `hybrid`: menggabungkan pertimbangan skor similarity dan frekuensi solusi.\n",
    "\n",
    "- **Supervised (TF-IDF + SVM)**\n",
    "  Sistem menggunakan model klasifikasi SVM untuk memprediksi `case_id` yang paling sesuai, lalu mengambil solusi dari kasus tersebut sebagai hasil prediksi.\n",
    "\n",
    "3. **Ringkasan Solusi**\n",
    "\n",
    "   - Solusi yang terpilih dipersingkat dengan mengambil 300 karakter pertama dari teks, sehingga menghasilkan ringkasan yang padat namun tetap informatif.\n",
    "\n",
    "4. **Uji Coba Sistem**\n",
    "\n",
    "   - Sistem dijalankan menggunakan 10 query uji yang terdapat pada file `queries.json` dari tahap sebelumnya.\n",
    "   - Untuk setiap query, digunakan dua fungsi: `predict_with_cosine()` dan `predict_with_svm()` guna menguji performa kedua pendekatan prediksi solusi.\n",
    "\n",
    "---\n",
    "\n",
    "#### Fungsi Utama\n",
    "\n",
    "- `predict_with_cosine(query: str, k=5, mode=\"weighted\")`\n",
    "  Melakukan pencarian terhadap *top-k* kasus paling relevan menggunakan cosine similarity, lalu menentukan solusi akhir dengan tiga opsi strategi:\n",
    "\n",
    "  - `\"weighted\"`: memilih solusi dari kasus dengan akumulasi skor similarity tertinggi.\n",
    "  - `\"majority\"`: memilih solusi yang paling sering muncul di antara hasil pencarian.\n",
    "  - `\"hybrid\"`: menggabungkan skor similarity dan frekuensi kemunculan solusi.\n",
    "\n",
    "- `predict_with_svm(query: str)`\n",
    "  Memprediksi `case_id` menggunakan model klasifikasi SVM, kemudian mengambil solusi dari kasus yang telah dipetakan tersebut.\n",
    "\n",
    "---\n",
    "\n",
    "#### Output\n",
    "\n",
    "Dua file CSV utama dihasilkan sebagai output dari proses prediksi:\n",
    "\n",
    "- `data/results/predictions_cosine.csv`\n",
    "  Berisi hasil prediksi solusi menggunakan pendekatan CBR dengan Cosine Similarity, menggunakan mode `weighted` sebagai pengaturan default.\n",
    "\n",
    "- `data/results/predictions_svm.csv`\n",
    "  Menyimpan hasil prediksi solusi berdasarkan model klasifikasi SVM.\n",
    "\n",
    "Setiap file memiliki kolom-kolom sebagai berikut:\n",
    "\n",
    "- `query_id`: Nomor identifikasi masing-masing query.\n",
    "- `query`: Ringkasan fakta dari kasus baru.\n",
    "- `predicted_solution`: Solusi yang dihasilkan berdasarkan kasus terdekat.\n",
    "- `top_5_case_ids`: Daftar lima case ID teratas hasil retrieval (untuk metode Cosine).\n",
    "- `top_1_case_id`: Case ID hasil prediksi tunggal dari model SVM.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12b1ca67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUKSES] Hasil prediksi COSINE disimpan di: data/results/predictions_cosine.csv\n",
      "[SUKSES] Hasil prediksi SVM disimpan di   : data/results/predictions_svm.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from collections import defaultdict, Counter\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# === PATH KONFIGURASI ===\n",
    "CASE_DATA = \"data/processed/cases_extracted.csv\"\n",
    "QUERY_PATH = \"data/eval/queries.json\"\n",
    "MODEL_PATH = \"03_retrieval_model.pkl\"\n",
    "VECTORIZER_PATH = \"03_vectorizer.pkl\"\n",
    "COSINE_OUTPUT = \"data/results/predictions_cosine.csv\"\n",
    "SVM_OUTPUT = \"data/results/predictions_svm.csv\"\n",
    "os.makedirs(\"data/results\", exist_ok=True)\n",
    "\n",
    "# === Load data dan model ===\n",
    "df = pd.read_csv(CASE_DATA)\n",
    "with open(QUERY_PATH, encoding=\"utf-8\") as f:\n",
    "    queries = json.load(f)\n",
    "\n",
    "classifier = joblib.load(MODEL_PATH)\n",
    "vectorizer = joblib.load(VECTORIZER_PATH)\n",
    "\n",
    "# === Siapkan korpus dokumen dan solusi ===\n",
    "texts = df[\"ringkasan_fakta\"].fillna(df[\"text_full\"]).astype(str).tolist()\n",
    "case_ids = df[\"case_id\"].astype(str).tolist()\n",
    "solutions = df[\"text_full\"].astype(str).tolist()\n",
    "\n",
    "tfidf_matrix = vectorizer.transform(texts)\n",
    "case_solutions = dict(zip(case_ids, solutions))\n",
    "case_id_to_text = dict(zip(case_ids, texts))\n",
    "\n",
    "# === COSINE SIMILARITY DENGAN MODE: majority / weighted / hybrid ===\n",
    "def predict_with_cosine(query: str, k: int = 5, mode: str = \"weighted\"):\n",
    "    query_vec = vectorizer.transform([query])\n",
    "    scores = cosine_similarity(query_vec, tfidf_matrix).flatten()\n",
    "    top_k_idx = scores.argsort()[::-1][:k]\n",
    "    \n",
    "    solution_scores = defaultdict(float)\n",
    "    solution_counts = Counter()\n",
    "    \n",
    "    for i in top_k_idx:\n",
    "        cid = case_ids[i]\n",
    "        score = float(scores[i])\n",
    "        solution = case_solutions.get(cid, \"\")[:300]\n",
    "\n",
    "        if mode == \"weighted\":\n",
    "            solution_scores[solution] += score\n",
    "        elif mode == \"majority\":\n",
    "            solution_counts[solution] += 1\n",
    "        elif mode == \"hybrid\":\n",
    "            solution_scores[solution] += score * 0.7 + solution_counts[solution] * 0.3\n",
    "        else:\n",
    "            raise ValueError(f\"Mode tidak dikenali: {mode}\")\n",
    "\n",
    "    # Pilih solusi terbaik\n",
    "    if mode in (\"weighted\", \"hybrid\"):\n",
    "        predicted_solution = max(solution_scores.items(), key=lambda x: x[1])[0]\n",
    "    else:  # majority\n",
    "        predicted_solution = solution_counts.most_common(1)[0][0]\n",
    "\n",
    "    top_ids = [case_ids[i] for i in top_k_idx]\n",
    "    return predicted_solution, top_ids\n",
    "\n",
    "# === SVM CLASSIFICATION ===\n",
    "def predict_with_svm(query: str):\n",
    "    query_vec = vectorizer.transform([query])\n",
    "    pred_case_id = classifier.predict(query_vec)[0]\n",
    "\n",
    "    sol = case_solutions.get(str(pred_case_id), \"\")\n",
    "    return sol[:300], [pred_case_id]\n",
    "\n",
    "# === Jalankan untuk semua query ===\n",
    "cosine_results = []\n",
    "svm_results = []\n",
    "\n",
    "for i, q in enumerate(queries, 1):\n",
    "    query_text = q[\"query\"]\n",
    "\n",
    "    # COSINE - mode: \"weighted\" | \"majority\" | \"hybrid\"\n",
    "    cosine_pred, cosine_ids = predict_with_cosine(query_text, mode=\"weighted\")\n",
    "    cosine_results.append({\n",
    "        \"query_id\": i,\n",
    "        \"query\": query_text,\n",
    "        \"predicted_solution\": cosine_pred,\n",
    "        \"top_5_case_ids\": cosine_ids\n",
    "    })\n",
    "\n",
    "    # SVM\n",
    "    svm_pred, svm_id = predict_with_svm(query_text)\n",
    "    svm_results.append({\n",
    "        \"query_id\": i,\n",
    "        \"query\": query_text,\n",
    "        \"predicted_solution\": svm_pred,\n",
    "        \"top_1_case_id\": svm_id[0]\n",
    "    })\n",
    "\n",
    "# === Simpan hasil ke CSV ===\n",
    "pd.DataFrame(cosine_results).to_csv(COSINE_OUTPUT, index=False)\n",
    "pd.DataFrame(svm_results).to_csv(SVM_OUTPUT, index=False)\n",
    "\n",
    "print(\"[SUKSES] Hasil prediksi COSINE disimpan di:\", COSINE_OUTPUT)\n",
    "print(\"[SUKSES] Hasil prediksi SVM disimpan di   :\", SVM_OUTPUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb781b4e",
   "metadata": {},
   "source": [
    "### Tahap 5 – Model Evaluation\n",
    "\n",
    "#### Tujuan\n",
    "\n",
    "Mengevaluasi performa sistem Case-Based Reasoning dalam memprediksi solusi hukum dengan membandingkan hasil prediksi dari pendekatan CBR berbasis Cosine Similarity dan klasifikasi SVM terhadap data uji, menggunakan metrik evaluasi seperti akurasi, precision, dan recall untuk menilai efektivitas masing-masing metode.\n",
    "\n",
    "---\n",
    "\n",
    "#### Langkah Kerja\n",
    "\n",
    "##### 1. Evaluasi Retrieval\n",
    "\n",
    "- Evaluasi difokuskan pada hasil *top-k retrieval* menggunakan pendekatan **TF-IDF + Cosine Similarity**.\n",
    "- Untuk setiap query, sistem memverifikasi apakah `ground_truth` terdapat di antara *top-k* kasus yang diambil.\n",
    "- Metrik yang digunakan untuk menilai kinerja meliputi:\n",
    "  - **Accuracy**\n",
    "  - **Precision**\n",
    "  - **Recall**\n",
    "  - **F1-Score**\n",
    "- Seluruh perhitungan dilakukan menggunakan fungsi-fungsi yang tersedia di modul `sklearn.metrics`.\n",
    "\n",
    "##### 2. Evaluasi Prediksi\n",
    "\n",
    "- Evaluasi dilakukan pada hasil prediksi tunggal (*top-1*) yang dihasilkan oleh pendekatan **TF-IDF + SVM**.\n",
    "- Setiap query menghasilkan satu `case_id` yang diprediksi, kemudian dibandingkan langsung dengan `ground_truth` untuk mengukur tingkat kesesuaian.\n",
    "- Metrik evaluasi yang digunakan tetap konsisten dengan evaluasi retrieval, yaitu:\n",
    "  - **Accuracy**\n",
    "  - **Precision**\n",
    "  - **Recall**\n",
    "  - **F1-Score**\n",
    "\n",
    "##### 3. Visualisasi & Laporan\n",
    "\n",
    "- Hasil evaluasi divisualisasikan dalam bentuk **grafik batang (bar chart)** untuk memperlihatkan perbandingan performa antar model secara jelas.\n",
    "- Dilakukan pula **analisis kesalahan (error analysis)** terhadap query-query yang prediksinya tidak sesuai dengan `ground_truth`, dan hasil analisis tersebut disimpan dalam format file JSON untuk keperluan peninjauan lebih lanjut.\n",
    "\n",
    "---\n",
    "\n",
    "#### Implementasi\n",
    "\n",
    "- Fungsi utama untuk evaluasi mencakup:\n",
    "\n",
    "  - `eval_retrieval()`: mengevaluasi performa pencarian top-k menggunakan cosine similarity.\n",
    "  - `eval_prediction()`: mengukur akurasi prediksi top-1 yang dihasilkan oleh model SVM.\n",
    "\n",
    "- Fungsi pendukung:\n",
    "\n",
    "  - `save_errors()`: mencatat dan menyimpan query yang hasil prediksinya tidak sesuai dengan ground truth.\n",
    "\n",
    "- Evaluasi dilakukan dengan mengacu pada file hasil prediksi berikut:\n",
    "\n",
    "  - `data/results/predictions_cosine.csv`\n",
    "  - `data/results/predictions_svm.csv`\n",
    "\n",
    "- File referensi jawaban yang benar (ground truth) terdapat pada:\n",
    "\n",
    "  - `data/eval/queries.json`\n",
    "\n",
    "---\n",
    "\n",
    "#### Output\n",
    "- **Output Evaluasi**\n",
    "\n",
    "  - `data/eval/retrieval_metrics.csv`\n",
    "    Berisi hasil pengukuran metrik evaluasi untuk pendekatan TF-IDF dengan Cosine Similarity.\n",
    "  - `data/eval/prediction_metrics.csv`\n",
    "    Berisi nilai metrik evaluasi untuk pendekatan TF-IDF dengan model SVM.\n",
    "\n",
    "- **Grafik Performa**\n",
    "\n",
    "  - `data/eval/performance_comparison.png`\n",
    "    Visualisasi berupa grafik batang yang membandingkan performa antar model berdasarkan metrik evaluasi.\n",
    "\n",
    "- **Analisis Kesalahan (Error Analysis)**\n",
    "\n",
    "  - `data/eval/error_cases_cosine.json`\n",
    "    Menyimpan daftar query yang tidak berhasil ditemukan secara akurat menggunakan metode cosine similarity.\n",
    "  - `data/eval/error_cases_svm.json`\n",
    "    Menyimpan query yang diprediksi salah oleh pendekatan berbasis SVM.\n",
    "\n",
    "---\n",
    "\n",
    "Tahap ini berperan penting dalam menilai sejauh mana sistem CBR efektif diterapkan pada kasus perdata khusus merek. Hasil evaluasi menjadi acuan utama dalam menentukan model terbaik yang akan digunakan untuk proses deployment maupun pengembangan sistem selanjutnya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8c8a1ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUKSES] Tahap 5 - Evaluasi Model selesai\n",
      "- Hasil evaluasi retrieval disimpan di: data/eval/retrieval_metrics.csv\n",
      "- Hasil evaluasi prediksi disimpan di : data/eval/prediction_metrics.csv\n",
      "- Visualisasi disimpan di             : data/eval/performance_comparison.png\n",
      "- Error cosine disimpan di           : data/eval/error_cases_cosine.json\n",
      "- Error svm disimpan di              : data/eval/error_cases_svm.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from typing import Dict\n",
    "\n",
    "# === PATH SETUP ===\n",
    "COSINE_PRED = \"data/results/predictions_cosine.csv\"\n",
    "SVM_PRED = \"data/results/predictions_svm.csv\"\n",
    "EVAL_QUERIES = \"data/eval/queries.json\"\n",
    "RETRIEVAL_METRIC_OUTPUT = \"data/eval/retrieval_metrics.csv\"\n",
    "PREDICTION_METRIC_OUTPUT = \"data/eval/prediction_metrics.csv\"\n",
    "ERROR_COSINE_OUTPUT = \"data/eval/error_cases_cosine.json\"\n",
    "ERROR_SVM_OUTPUT = \"data/eval/error_cases_svm.json\"\n",
    "os.makedirs(\"data/eval\", exist_ok=True)\n",
    "\n",
    "# === LOAD GROUND TRUTH ===\n",
    "with open(EVAL_QUERIES, encoding='utf-8') as f:\n",
    "    ground_truth_data = json.load(f)\n",
    "    gt_dict = {i + 1: str(item[\"ground_truth\"]).strip() for i, item in enumerate(ground_truth_data)}\n",
    "\n",
    "# === EVALUASI RETRIEVAL COSINE (TOP-K) ===\n",
    "def eval_retrieval(pred_file: str, model_name: str, k: int = 5) -> Dict:\n",
    "    df = pd.read_csv(pred_file)\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        query_id = int(row[\"query_id\"])\n",
    "        gt = str(gt_dict.get(query_id, \"\")).strip()\n",
    "\n",
    "        try:\n",
    "            pred_ids = eval(row[\"top_5_case_ids\"])\n",
    "            if not isinstance(pred_ids, list):\n",
    "                pred_ids = []\n",
    "            pred_ids = [str(i).strip() for i in pred_ids]\n",
    "        except Exception:\n",
    "            pred_ids = []\n",
    "\n",
    "        hit = gt in pred_ids[:k]\n",
    "        y_true.append(1)\n",
    "        y_pred.append(1 if hit else 0)\n",
    "\n",
    "    return {\n",
    "        \"model\": model_name,\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"precision\": precision_score(y_true, y_pred, zero_division=0),\n",
    "        \"recall\": recall_score(y_true, y_pred, zero_division=0),\n",
    "        \"f1_score\": f1_score(y_true, y_pred, zero_division=0)\n",
    "    }\n",
    "\n",
    "# === EVALUASI SVM PREDICTION (TOP-1) ===\n",
    "def eval_prediction(pred_file: str, model_name: str) -> Dict:\n",
    "    df = pd.read_csv(pred_file)\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        query_id = int(row[\"query_id\"])\n",
    "        gt = str(gt_dict.get(query_id, \"\")).strip()\n",
    "        pred = str(row.get(\"top_1_case_id\", \"\")).strip()\n",
    "\n",
    "        if not pred:\n",
    "            continue\n",
    "\n",
    "        hit = pred == gt\n",
    "        y_true.append(1)\n",
    "        y_pred.append(1 if hit else 0)\n",
    "\n",
    "    if not y_true:\n",
    "        print(f\"[PERINGATAN] Tidak ada prediksi valid untuk model {model_name}\")\n",
    "        return {\n",
    "            \"model\": model_name,\n",
    "            \"accuracy\": 0.0,\n",
    "            \"precision\": 0.0,\n",
    "            \"recall\": 0.0,\n",
    "            \"f1_score\": 0.0\n",
    "        }\n",
    "\n",
    "    return {\n",
    "        \"model\": model_name,\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"precision\": precision_score(y_true, y_pred, zero_division=0),\n",
    "        \"recall\": recall_score(y_true, y_pred, zero_division=0),\n",
    "        \"f1_score\": f1_score(y_true, y_pred, zero_division=0)\n",
    "    }\n",
    "\n",
    "# === SIMPAN KASUS GAGAL ===\n",
    "def save_errors(pred_file: str, output_file: str, top_k: int = 5, mode: str = 'cosine'):\n",
    "    df = pd.read_csv(pred_file)\n",
    "    error_cases = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        query_id = int(row[\"query_id\"])\n",
    "        gt = str(gt_dict.get(query_id, \"\")).strip()\n",
    "\n",
    "        if mode == 'cosine':\n",
    "            try:\n",
    "                pred_ids = eval(row[\"top_5_case_ids\"])\n",
    "                if not isinstance(pred_ids, list):\n",
    "                    pred_ids = []\n",
    "                pred_ids = [str(i).strip() for i in pred_ids]\n",
    "            except Exception:\n",
    "                pred_ids = []\n",
    "            hit = gt in pred_ids[:top_k]\n",
    "        else:\n",
    "            pred_ids = [str(row.get(\"top_1_case_id\", \"\")).strip()]\n",
    "            hit = gt == pred_ids[0]\n",
    "\n",
    "        if not hit:\n",
    "            error_cases.append({\n",
    "                \"query_id\": query_id,\n",
    "                \"query\": row[\"query\"],\n",
    "                \"predicted\": pred_ids,\n",
    "                \"ground_truth\": gt\n",
    "            })\n",
    "\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(error_cases, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# === EKSEKUSI & SIMPAN HASIL ===\n",
    "retrieval_metrics = eval_retrieval(COSINE_PRED, \"TF-IDF + Cosine\")\n",
    "prediction_metrics = eval_prediction(SVM_PRED, \"TF-IDF + SVM\")\n",
    "\n",
    "pd.DataFrame([retrieval_metrics]).to_csv(RETRIEVAL_METRIC_OUTPUT, index=False)\n",
    "pd.DataFrame([prediction_metrics]).to_csv(PREDICTION_METRIC_OUTPUT, index=False)\n",
    "\n",
    "save_errors(COSINE_PRED, ERROR_COSINE_OUTPUT, mode='cosine')\n",
    "save_errors(SVM_PRED, ERROR_SVM_OUTPUT, mode='svm')\n",
    "\n",
    "# === VISUALISASI ===\n",
    "combined_df = pd.DataFrame([retrieval_metrics, prediction_metrics])\n",
    "if not combined_df.empty:\n",
    "    combined_df.set_index(\"model\")[[\"accuracy\", \"precision\", \"recall\", \"f1_score\"]].plot(\n",
    "        kind=\"bar\", figsize=(10, 6), title=\"Perbandingan Model\"\n",
    "    )\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"data/eval/performance_comparison.png\")\n",
    "    plt.close()\n",
    "\n",
    "print(\"[SUKSES] Tahap 5 - Evaluasi Model selesai\")\n",
    "print(f\"- Hasil evaluasi retrieval disimpan di: {RETRIEVAL_METRIC_OUTPUT}\")\n",
    "print(f\"- Hasil evaluasi prediksi disimpan di : {PREDICTION_METRIC_OUTPUT}\")\n",
    "print(f\"- Visualisasi disimpan di             : data/eval/performance_comparison.png\")\n",
    "print(f\"- Error cosine disimpan di           : {ERROR_COSINE_OUTPUT}\")\n",
    "print(f\"- Error svm disimpan di              : {ERROR_SVM_OUTPUT}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148eec78",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
